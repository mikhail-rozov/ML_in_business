{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd0e2ee6-8415-44a6-90c9-10de9548916d",
   "metadata": {},
   "source": [
    "## Практическое задание к уроку № 2 по теме \"Профилирование пользователей. Сегментация.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca9489e-8f98-4a8e-91fc-1e41949c1095",
   "metadata": {},
   "source": [
    "1. *Самостоятельно повторить tfidf (документация https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "2. Модифицировать код функции get_user_embedding таким образом, чтобы считалось не среднее (как в примере np.mean), а медиана. Применить такое преобразование к данным, обучить модель прогнозирования оттока и посчитать метрики качества и сохранить их: roc auc, precision/recall/f_score (для 3 последних - подобрать оптимальный порог)\n",
    "3. Повторить п.2, но используя уже не медиану, а max\n",
    "4. *Воспользовавшись полученными знаниями из п.1, повторить пункт 2, но уже взвешивая новости по tfidf (взяв список новостей пользователя)\n",
    "    - подсказка 1: нужно получить веса-коэффициенты для каждого документа. Не все документы одинаково информативны и несут какой-то положительный сигнал\n",
    "    - подсказка 2: нужен именно idf, как вес.\n",
    "5. Сформировать на выходе единую таблицу, сравнивающую качество 2/3 разных метода получения эмбедингов пользователей: median, max, idf_mean по метрикам roc_auc, precision, recall, f_score\n",
    "6. Сделать самостоятельные выводы и предположения о том, почему тот или ной способ оказался эффективнее остальных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c6f6cb-dff3-42a1-9a6b-a6b843d39e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.models import LdaModel\n",
    "from gensim.test.utils import datapath\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymorphy2\n",
    "from razdel import tokenize\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (f1_score, roc_auc_score, precision_score, average_precision_score,\n",
    "                             classification_report, precision_recall_curve, confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4959f3f8-b7cb-4866-930f-30b2c3bfa89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Заместитель председателяnправительства РФnСерг...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>Матч 1/16 финала Кубка России по футболу был п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>Форвард «Авангарда» Томаш Заборский прокоммент...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                              title\n",
       "0       6  Заместитель председателяnправительства РФnСерг...\n",
       "1    4896  Матч 1/16 финала Кубка России по футболу был п...\n",
       "2    4897  Форвард «Авангарда» Томаш Заборский прокоммент..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv(\"articles.csv\")\n",
    "print(news.shape)\n",
    "news.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f22f6048-6a50-47cc-bebf-9490bddcadf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u105138</td>\n",
       "      <td>[293672, 293328, 293001, 293622, 293126, 1852]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u108690</td>\n",
       "      <td>[3405, 1739, 2972, 1158, 1599, 322665]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u108339</td>\n",
       "      <td>[1845, 2009, 2356, 1424, 2939, 323389]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid                                        articles\n",
       "0  u105138  [293672, 293328, 293001, 293622, 293126, 1852]\n",
       "1  u108690          [3405, 1739, 2972, 1158, 1599, 322665]\n",
       "2  u108339          [1845, 2009, 2356, 1424, 2939, 323389]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = pd.read_csv(\"users_articles.csv\")\n",
    "users.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d8c9e27-d0f0-463e-8f6d-33732723652b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shkin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopword_ru = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86976ac1-7797-4282-a7f9-93dac1e16208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopword_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04021c4e-94a2-4272-93c6-5c9bd20a996c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('stopwords.txt', encoding='utf-8') as f:\n",
    "    additional_stopwords = [w.strip() for w in f.readlines() if w]\n",
    "    \n",
    "stopword_ru += additional_stopwords\n",
    "len(stopword_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cde1ac99-2f46-428f-a81e-4246bc42b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    очистка текста\n",
    "    \n",
    "    на выходе очищеный текст\n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
    "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
    "\n",
    "    text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
    "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
    "    text = re.sub('n', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "cache = {}\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def lemmatization(text):    \n",
    "    '''\n",
    "    лемматизация\n",
    "        [0] если зашел тип не `str` делаем его `str`\n",
    "        [1] токенизация предложения через razdel\n",
    "        [2] проверка есть ли в начале слова '-'\n",
    "        [3] проверка токена с одного символа\n",
    "        [4] проверка есть ли данное слово в кэше\n",
    "        [5] лемматизация слова\n",
    "        [6] проверка на стоп-слова\n",
    "\n",
    "    на выходе лист лемматизированых токенов\n",
    "    '''\n",
    "\n",
    "    # [0]\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # [1]\n",
    "    tokens = list(tokenize(text))\n",
    "    words = [_.text for _ in tokens]\n",
    "\n",
    "    words_lem = []\n",
    "    for w in words:\n",
    "        if w[0] == '-': # [2]\n",
    "            w = w[1:]\n",
    "        if len(w) > 1: # [3]\n",
    "            if w in cache: # [4]\n",
    "                words_lem.append(cache[w])\n",
    "            else: # [5]\n",
    "                temp_cach = cache[w] = morph.parse(w)[0].normal_form\n",
    "                words_lem.append(temp_cach)\n",
    "    \n",
    "    words_lem_without_stopwords = [i for i in words_lem if not i in stopword_ru] # [6]\n",
    "    \n",
    "    return words_lem_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "588c0cdd-bed9-4e97-b6bd-fbfc081c5e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27000 [00:00<?, ?it/s]C:\\Users\\Shkin\\AppData\\Local\\Temp\\ipykernel_1496\\4083466619.py:14: FutureWarning: Possible nested set at position 39\n",
      "  text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
      "100%|██████████| 27000/27000 [00:19<00:00, 1359.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 19.9 s\n",
      "Wall time: 19.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tqdm.pandas()\n",
    "# Запускаем очистку текста\n",
    "news['title'] = news['title'].progress_apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a85a0bf-94c0-42be-9b14-032a7ce63ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    заместитель председателя правительства рф серг...\n",
       "1    матч  финала кубка россии по футболу был приос...\n",
       "2    форвард авангарда томаш заборский прокомментир...\n",
       "3    главный тренер кубани юрий красножан прокоммен...\n",
       "4    решением попечительского совета владивостокско...\n",
       "5    ио главного тренера вячеслав буцаев прокоммент...\n",
       "6    запорожский металлург дома потерпел разгромное...\n",
       "7    сборная сша одержала победу над австрией со сч...\n",
       "8    бывший защитник сборной россии дарюс каспарайт...\n",
       "9    полузащитник цска зоран тошич после победы над...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news['title'].iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c45552c-dd2f-48be-a357-e8904f6a7071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27000/27000 [02:15<00:00, 199.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 15s\n",
      "Wall time: 2min 15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Запускаем лемматизацию текста\n",
    "news['title'] = news['title'].progress_apply(lambda x: lemmatization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48eed3f6-9d53-401f-8a3f-ac67d5995341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сформируем список наших текстов\n",
    "texts = list(news['title'].values)\n",
    "\n",
    "# Создадим корпус из списка с текстами\n",
    "common_dictionary = Dictionary(texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326deae8-0a30-41c8-b5bf-0ba57e7ba814",
   "metadata": {},
   "source": [
    "Запускаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d155151f-c820-4851-8b00-7532982df28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_topic = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4aa46c9-3aa1-4a81-a92f-5f6d0b6892fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 35.6 s\n",
      "Wall time: 32.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Обучаем модель на корпусе\n",
    "lda = LdaModel(common_corpus, num_topics=N_topic, id2word=common_dictionary, random_state=29)#, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0184da92-de51-40f6-8c50-8f33c7533720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем модель на диск\n",
    "temp_file = datapath(\"model.lda\")\n",
    "lda.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cc04c23-8d60-4512-9c53-1aab14ca50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем обученную модель с диска\n",
    "lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "377c08fc-1e9e-48fd-9c68-2af96de90407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_0: млрд рубль составить млн бюджет ставка рост\n",
      "topic_1: тело обнаружить взрыв выяснить сша газ космос\n",
      "topic_2: автор свидетель лётчик выделить диапазон умереть фаза\n",
      "topic_3: ракета снижение запуск энергия испытание израиль брюссель\n",
      "topic_4: статья кровь земля свет способность кость вход\n",
      "topic_5: предприниматель египетский гражданство sa концепция пилотировать ведение\n",
      "topic_6: поверхность восток германия египет иран европа франция\n",
      "topic_7: место рейтинг температура россиянин первый вода млн\n",
      "topic_8: россия сша рынок российский цена рост всё\n",
      "topic_9: исследование журнал всё женщина газета день писать\n",
      "topic_10: гражданин остров конкурс фронт народный памятник супруг\n",
      "topic_11: ребёнок жизнь смерть возраст автор организм советский\n",
      "topic_12: газ участок торговый площадь глава москва задержать\n",
      "topic_13: погибнуть фонд выяснить доллар дыра вирус воздух\n",
      "topic_14: военный сша эксперимент земля рак армия северный\n",
      "topic_15: российский россия банк население решение сторона объём\n",
      "topic_16: россия закон путин рубль белоруссия законопроект статья\n",
      "topic_17: новый проект эксперт научный украина наука россия\n",
      "topic_18: су таиланд представляться герман мышь жуковский медвежий\n",
      "topic_19: мозг продукция общество китай путин километр китайский\n"
     ]
    }
   ],
   "source": [
    "x = lda.show_topics(num_topics=N_topic, num_words=7, formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "# Печатаем только слова\n",
    "for topic, words in topics_words:\n",
    "    print(f\"topic_{topic}: \" + \" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f064e648-e1e8-43e7-ba10-9c72263666b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_vector(lda, text):\n",
    "\n",
    "    unseen_doc = common_dictionary.doc2bow(text)\n",
    "    lda_tuple = lda[unseen_doc]\n",
    "\n",
    "    not_null_topics = dict(lda_tuple)\n",
    "\n",
    "    output_vector = []\n",
    "    for i in range(N_topic):\n",
    "        output_vector.append(not_null_topics[i] if i in not_null_topics else 0)\n",
    "        \n",
    "    return np.array(output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "516e0271-c63d-43fd-bd58-fa12607c65c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 27.2 s\n",
      "Wall time: 24.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "topic_matrix = pd.DataFrame([get_lda_vector(lda, text) for text in news['title'].values])\n",
    "topic_matrix.columns = [f'topic_{i}' for i in range(N_topic)]\n",
    "topic_matrix['doc_id'] = news['doc_id'].values\n",
    "topic_matrix = topic_matrix[['doc_id']+[f'topic_{i}' for i in range(N_topic)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78a238f7-a7ff-4243-b97e-eebc27551597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>topic_11</th>\n",
       "      <th>topic_12</th>\n",
       "      <th>topic_13</th>\n",
       "      <th>topic_14</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.891777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.390715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095284</td>\n",
       "      <td>0.136507</td>\n",
       "      <td>0.030956</td>\n",
       "      <td>0.199992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193319</td>\n",
       "      <td>0.116941</td>\n",
       "      <td>0.104652</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086819</td>\n",
       "      <td>0.110570</td>\n",
       "      <td>0.221759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.507816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.077323</td>\n",
       "      <td>0.244675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id  topic_0   topic_1   topic_2   topic_3   topic_4  topic_5   topic_6  \\\n",
       "0       6      0.0  0.000000  0.000000  0.000000  0.000000      0.0  0.000000   \n",
       "1    4896      0.0  0.585589  0.000000  0.000000  0.000000      0.0  0.390715   \n",
       "2    4897      0.0  0.095284  0.136507  0.030956  0.199992      0.0  0.193319   \n",
       "3    4898      0.0  0.000000  0.000000  0.000000  0.000000      0.0  0.086819   \n",
       "4    4899      0.0  0.000000  0.000000  0.058675  0.000000      0.0  0.000000   \n",
       "\n",
       "    topic_7   topic_8  ...  topic_10  topic_11  topic_12  topic_13  topic_14  \\\n",
       "0  0.042943  0.000000  ...       0.0  0.000000  0.056732       0.0     0.000   \n",
       "1  0.000000  0.000000  ...       0.0  0.000000  0.000000       0.0     0.000   \n",
       "2  0.116941  0.104652  ...       0.0  0.000000  0.000000       0.0     0.000   \n",
       "3  0.110570  0.221759  ...       0.0  0.000000  0.000000       0.0     0.000   \n",
       "4  0.000000  0.000000  ...       0.0  0.077323  0.244675       0.0     0.053   \n",
       "\n",
       "   topic_15  topic_16  topic_17  topic_18  topic_19  \n",
       "0       0.0       0.0  0.891777       0.0  0.000000  \n",
       "1       0.0       0.0  0.000000       0.0  0.000000  \n",
       "2       0.0       0.0  0.000000       0.0  0.000000  \n",
       "3       0.0       0.0  0.063256       0.0  0.507816  \n",
       "4       0.0       0.0  0.428206       0.0  0.000000  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40384491-a77e-4c46-a8a0-9998bdc7250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = dict(zip(topic_matrix['doc_id'].values, topic_matrix[[f'topic_{i}' for i in range(N_topic)]].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "084aca5a-843b-4301-8b39-cdcb1f4fa938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.13186674, 0.        , 0.09756994,\n",
       "       0.        , 0.        , 0.        , 0.29824179, 0.        ,\n",
       "       0.        , 0.07639583, 0.37508217, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_dict[293672]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa6f447e-7efb-4549-965a-2b4e83b35316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_embedding(user_articles_list, doc_dict, func='mean'):\n",
    "    user_articles_list = eval(user_articles_list)\n",
    "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "    \n",
    "    match func:\n",
    "        case 'mean':\n",
    "            user_vector = np.mean(user_vector, axis=0)\n",
    "        case 'median':\n",
    "            user_vector = np.median(user_vector, axis=0)\n",
    "        case 'max':\n",
    "            user_vector = np.max(user_vector, axis=0)\n",
    "            \n",
    "    return user_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a115463-4aca-46c9-8e0c-4c5f06667069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u107120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u102277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u102444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid  churn\n",
       "0  u107120      0\n",
       "1  u102277      0\n",
       "2  u102444      0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.read_csv(\"users_churn.csv\")\n",
    "target.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38e1240c-9151-4704-a33e-cd0213de7d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embds = []\n",
    "for func in ('mean', 'median', 'max'):\n",
    "    user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x, doc_dict, func=func))])\n",
    "    user_embeddings.columns = [f'topic_{i}' for i in range(N_topic)]\n",
    "    user_embeddings['uid'] = users['uid'].values\n",
    "    user_embeddings = user_embeddings[['uid']+[f'topic_{i}' for i in range(N_topic)]]\n",
    "    embds.append(user_embeddings)\n",
    "    \n",
    "user_embeddings_mean = embds[0]\n",
    "user_embeddings_median = embds[1]\n",
    "user_embeddings_max = embds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa00b861-dbaf-4ec4-83c2-34ec56430290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>topic_11</th>\n",
       "      <th>topic_12</th>\n",
       "      <th>topic_13</th>\n",
       "      <th>topic_14</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u105138</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.025017</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038701</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.017695</td>\n",
       "      <td>0.026245</td>\n",
       "      <td>0.135578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039980</td>\n",
       "      <td>0.155588</td>\n",
       "      <td>0.081138</td>\n",
       "      <td>0.005453</td>\n",
       "      <td>0.020918</td>\n",
       "      <td>0.125869</td>\n",
       "      <td>0.027897</td>\n",
       "      <td>0.151842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u108690</td>\n",
       "      <td>0.037412</td>\n",
       "      <td>0.003313</td>\n",
       "      <td>0.005821</td>\n",
       "      <td>0.003530</td>\n",
       "      <td>0.014091</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008299</td>\n",
       "      <td>0.197037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.060942</td>\n",
       "      <td>0.050219</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.051332</td>\n",
       "      <td>0.203402</td>\n",
       "      <td>0.059888</td>\n",
       "      <td>0.171751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u108339</td>\n",
       "      <td>0.014135</td>\n",
       "      <td>0.024985</td>\n",
       "      <td>0.007149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>0.00204</td>\n",
       "      <td>0.003104</td>\n",
       "      <td>0.034605</td>\n",
       "      <td>0.056011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014784</td>\n",
       "      <td>0.024339</td>\n",
       "      <td>0.077272</td>\n",
       "      <td>0.025605</td>\n",
       "      <td>0.058922</td>\n",
       "      <td>0.227227</td>\n",
       "      <td>0.111113</td>\n",
       "      <td>0.199366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u101138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.033607</td>\n",
       "      <td>0.014503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.068481</td>\n",
       "      <td>0.215124</td>\n",
       "      <td>0.135114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031991</td>\n",
       "      <td>0.017394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035167</td>\n",
       "      <td>0.021153</td>\n",
       "      <td>0.037008</td>\n",
       "      <td>0.114231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u108248</td>\n",
       "      <td>0.026361</td>\n",
       "      <td>0.032143</td>\n",
       "      <td>0.006323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014334</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.059714</td>\n",
       "      <td>0.161864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005953</td>\n",
       "      <td>0.052074</td>\n",
       "      <td>0.022068</td>\n",
       "      <td>0.009924</td>\n",
       "      <td>0.057653</td>\n",
       "      <td>0.113160</td>\n",
       "      <td>0.079171</td>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid   topic_0   topic_1   topic_2   topic_3   topic_4  topic_5  \\\n",
       "0  u105138  0.002669  0.025017  0.021978  0.000000  0.038701  0.00000   \n",
       "1  u108690  0.037412  0.003313  0.005821  0.003530  0.014091  0.00000   \n",
       "2  u108339  0.014135  0.024985  0.007149  0.000000  0.004763  0.00204   \n",
       "3  u101138  0.000000  0.036439  0.033607  0.014503  0.000000  0.00000   \n",
       "4  u108248  0.026361  0.032143  0.006323  0.000000  0.014334  0.00000   \n",
       "\n",
       "    topic_6   topic_7   topic_8  ...  topic_10  topic_11  topic_12  topic_13  \\\n",
       "0  0.017695  0.026245  0.135578  ...  0.039980  0.155588  0.081138  0.005453   \n",
       "1  0.000000  0.008299  0.197037  ...  0.008439  0.060942  0.050219  0.001755   \n",
       "2  0.003104  0.034605  0.056011  ...  0.014784  0.024339  0.077272  0.025605   \n",
       "3  0.068481  0.215124  0.135114  ...  0.000000  0.031991  0.017394  0.000000   \n",
       "4  0.004212  0.059714  0.161864  ...  0.005953  0.052074  0.022068  0.009924   \n",
       "\n",
       "   topic_14  topic_15  topic_16  topic_17  topic_18  topic_19  \n",
       "0  0.020918  0.125869  0.027897  0.151842       0.0  0.009785  \n",
       "1  0.051332  0.203402  0.059888  0.171751       0.0  0.007435  \n",
       "2  0.058922  0.227227  0.111113  0.199366       0.0  0.020610  \n",
       "3  0.035167  0.021153  0.037008  0.114231       0.0  0.220260  \n",
       "4  0.057653  0.113160  0.079171  0.157355       0.0  0.021711  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_embeddings_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f0beafb-438b-4e53-b014-0ac9f0ec918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(user_embeddings, target, scaled=False, plot=False):\n",
    "\n",
    "    X = pd.merge(user_embeddings, target, 'left')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.drop(['uid', 'churn'], axis=1), \n",
    "                                                    X['churn'], random_state=29)\n",
    "    \n",
    "    if scaled:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    if plot:\n",
    "        n = 50\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(preds[:n], label='predict')\n",
    "        plt.plot(y_test.values[:n], label='true')\n",
    "        plt.title('ответ модели')\n",
    "        plt.xlabel('№ примера')\n",
    "        plt.ylabel('выход')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "    fscore = []\n",
    "    for i in range(len(precision)):\n",
    "        if (precision[i] + recall[i]) != 0:\n",
    "            fscore.append((2 * precision[i] * recall[i]) / (precision[i] + recall[i]))\n",
    "        else:\n",
    "            fscore.append(0)\n",
    "            \n",
    "    ix = np.argmax(np.array(fscore))\n",
    "    f_score_ = round(fscore[ix], 3)\n",
    "    precision_ = round(precision[ix], 3)\n",
    "    recall_ = round(recall[ix], 3)\n",
    "    roc_auc_score_ = round(roc_auc_score(y_test, preds), 3)\n",
    "    ap_score_ = round(average_precision_score(y_test, preds), 3)\n",
    "    print(f'F-score:\\t{f_score_}\\n'\n",
    "          f'Precision:\\t{precision_}\\n'\n",
    "          f'Recall:\\t\\t{recall_}\\n'\n",
    "          f'ROC-AUC score:\\t{roc_auc_score_}\\n'\n",
    "          f'AP-score:\\t{ap_score_}')\n",
    "        \n",
    "    return f_score_, precision_, recall_, roc_auc_score_, ap_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432a958-b131-4f90-aded-ef7d9670bfbe",
   "metadata": {},
   "source": [
    "#### Mean, no tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9f9e8fb-d9d8-4f76-9c91-d0d0ca916530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score:\t0.634\n",
      "Precision:\t0.559\n",
      "Recall:\t\t0.733\n",
      "ROC-AUC score:\t0.923\n",
      "AP-score:\t0.626\n"
     ]
    }
   ],
   "source": [
    "f_score_mean, precision_mean, recall_mean, roc_auc_score_mean, ap_score_mean = get_scores(user_embeddings_mean, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b968e38-3c2f-4371-a087-9a8c51045644",
   "metadata": {},
   "source": [
    "#### Median, no tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0488c5d7-8cd2-48c6-97b8-f3a60950bba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score:\t0.71\n",
      "Precision:\t0.614\n",
      "Recall:\t\t0.842\n",
      "ROC-AUC score:\t0.958\n",
      "AP-score:\t0.77\n"
     ]
    }
   ],
   "source": [
    "f_score_median, precision_median, recall_median, roc_auc_score_median, ap_score_median = get_scores(user_embeddings_median, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940c95e1-f009-4964-8302-549e38d58ef8",
   "metadata": {},
   "source": [
    "#### Max, no tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff62389c-9296-4239-9e6d-5155fb6ce487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score:\t0.745\n",
      "Precision:\t0.718\n",
      "Recall:\t\t0.773\n",
      "ROC-AUC score:\t0.962\n",
      "AP-score:\t0.816\n"
     ]
    }
   ],
   "source": [
    "f_score_max, precision_max, recall_max, roc_auc_score_max, ap_score_max = get_scores(user_embeddings_max, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd724d2f-276b-4b18-800b-cf384b69ee5f",
   "metadata": {},
   "source": [
    "### tfidf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db396b7f-0f9e-4fbf-b81e-10ad27a95fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_tfidf = [' '.join(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0811651-29d8-4faf-9e4e-4018e473b758",
   "metadata": {},
   "source": [
    "Обучим на уже обработанном тексте, используя имеющийся словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0198e6f9-6a26-439f-922e-1c4d5c639aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(vocabulary={'aa': 60397, 'aaa': 102346, 'aaas': 135467,\n",
       "                            'aabar': 68677, 'aacsb': 97861, 'aad': 51893,\n",
       "                            'aamal': 102225, 'aamaq': 71366, 'aami': 76620,\n",
       "                            'aaplo': 96784, 'aaq': 59120, 'aar': 90219,\n",
       "                            'aarata': 103513, 'aarhus': 134368, 'aaro': 64137,\n",
       "                            'aatip': 128768, 'aatsa': 98251, 'ab': 51755,\n",
       "                            'aba': 38521, 'ababeel': 60163, 'abalsslv': 73069,\n",
       "                            'abba': 29215, 'abbey': 32026, 'abbott': 105400,\n",
       "                            'abbraccio': 75134, 'abbvie': 113001, 'abby': 99106,\n",
       "                            'abbyy': 38966, 'abc': 16452, 'abccomlive': 95641, ...})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TfidfVectorizer(vocabulary=common_dictionary.token2id)\n",
    "tf.fit(texts_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69313d05-375f-47da-85c9-03dbb0c9957e",
   "metadata": {},
   "source": [
    "Добавим веса словам, а затем и темам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7aa18a11-0493-4212-8150-86459d97bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dict = {}\n",
    "for word, idx in common_dictionary.token2id.items():\n",
    "    weights_dict[word] = tf.idf_[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08d171bf-a15d-4551-bba3-29789096cbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_weights = []\n",
    "for topic in topics_words:\n",
    "    weight = np.mean([weights_dict[word] for word in topic[1]])\n",
    "    topics_weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5b5a892-0b04-40e2-8f80-754f48146cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.426062221135475,\n",
       " 3.976909277630117,\n",
       " 4.811976917613817,\n",
       " 4.576623806870413,\n",
       " 4.566009741992158]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_weights[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cf2da0-b275-4d0a-8eea-ff8d42641722",
   "metadata": {},
   "source": [
    "Найдём веса документов, умножив веса тем на вероятности (доли) тем в каждом документе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "babda518-7fa8-46ee-9fcb-e81b99ff941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_matrix_tfidf = topic_matrix.copy()\n",
    "doc_weights = sum(topic_matrix_tfidf[f'topic_{i}'] * topics_weights[i] for i in range(N_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f623b24-82e4-48ae-8d41-d6e0c6b0df4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27000,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72707706-6a47-4c73-b3ba-b5ada792316d",
   "metadata": {},
   "source": [
    "Получим взвешенные векторы документов, умножив их на веса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d4e1bb5-c737-45df-a4d9-e8650d029587",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N_topic):\n",
    "    topic_matrix_tfidf[f'topic_{i}'] *= doc_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22de7208-a0fd-459c-b168-b64aa34a4747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>topic_11</th>\n",
       "      <th>topic_12</th>\n",
       "      <th>topic_13</th>\n",
       "      <th>topic_14</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.711991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.326160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.552054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.370395</td>\n",
       "      <td>0.530643</td>\n",
       "      <td>0.120335</td>\n",
       "      <td>0.777428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.751486</td>\n",
       "      <td>0.454583</td>\n",
       "      <td>0.406811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.319524</td>\n",
       "      <td>0.406938</td>\n",
       "      <td>0.816152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.868945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.254019</td>\n",
       "      <td>0.803796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.406721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id  topic_0   topic_1   topic_2   topic_3   topic_4  topic_5   topic_6  \\\n",
       "0       6      0.0  0.000000  0.000000  0.000000  0.000000      0.0  0.000000   \n",
       "1    4896      0.0  2.326160  0.000000  0.000000  0.000000      0.0  1.552054   \n",
       "2    4897      0.0  0.370395  0.530643  0.120335  0.777428      0.0  0.751486   \n",
       "3    4898      0.0  0.000000  0.000000  0.000000  0.000000      0.0  0.319524   \n",
       "4    4899      0.0  0.000000  0.000000  0.192758  0.000000      0.0  0.000000   \n",
       "\n",
       "    topic_7   topic_8  ...  topic_10  topic_11  topic_12  topic_13  topic_14  \\\n",
       "0  0.130596  0.000000  ...       0.0  0.000000  0.172528       0.0  0.000000   \n",
       "1  0.000000  0.000000  ...       0.0  0.000000  0.000000       0.0  0.000000   \n",
       "2  0.454583  0.406811  ...       0.0  0.000000  0.000000       0.0  0.000000   \n",
       "3  0.406938  0.816152  ...       0.0  0.000000  0.000000       0.0  0.000000   \n",
       "4  0.000000  0.000000  ...       0.0  0.254019  0.803796       0.0  0.174114   \n",
       "\n",
       "   topic_15  topic_16  topic_17  topic_18  topic_19  \n",
       "0       0.0       0.0  2.711991       0.0  0.000000  \n",
       "1       0.0       0.0  0.000000       0.0  0.000000  \n",
       "2       0.0       0.0  0.000000       0.0  0.000000  \n",
       "3       0.0       0.0  0.232804       0.0  1.868945  \n",
       "4       0.0       0.0  1.406721       0.0  0.000000  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_matrix_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab97bd9-50b2-4b44-9b05-36e9d0a1c850",
   "metadata": {},
   "source": [
    "Повторим операции для нового датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b082042-e823-4766-8bff-d64cfec8a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict_tfidf = dict(zip(topic_matrix_tfidf['doc_id'].values, topic_matrix_tfidf[[f'topic_{i}' for i in range(N_topic)]].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a06ebb09-c1ca-4ca3-858f-942378870272",
   "metadata": {},
   "outputs": [],
   "source": [
    "embds_tfidf = []\n",
    "for func in ('mean', 'median', 'max'):\n",
    "    user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x, doc_dict_tfidf, func=func))])\n",
    "    user_embeddings.columns = [f'topic_{i}' for i in range(N_topic)]\n",
    "    user_embeddings['uid'] = users['uid'].values\n",
    "    user_embeddings = user_embeddings[['uid']+[f'topic_{i}' for i in range(N_topic)]]\n",
    "    embds_tfidf.append(user_embeddings)\n",
    "    \n",
    "user_embeddings_mean_tfidf = embds_tfidf[0]\n",
    "user_embeddings_median_tfidf = embds_tfidf[1]\n",
    "user_embeddings_max_tfidf = embds_tfidf[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21df4d1-0bd2-45d4-a624-0f75bec129e6",
   "metadata": {},
   "source": [
    "#### Mean, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1398029e-e4a9-4e3d-934b-f7bb1fb61440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score:\t0.665\n",
      "Precision:\t0.648\n",
      "Recall:\t\t0.684\n",
      "ROC-AUC score:\t0.936\n",
      "AP-score:\t0.68\n"
     ]
    }
   ],
   "source": [
    "f_score_mean_tfidf, precision_mean_tfidf, recall_mean_tfidf, roc_auc_score_mean_tfidf, ap_score_mean_tfidf = get_scores(user_embeddings_mean_tfidf, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb0498-c20e-418b-8597-9000d169c26d",
   "metadata": {},
   "source": [
    "#### Median, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "346c96a3-226c-4dea-a152-7cad249036ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score:\t0.742\n",
      "Precision:\t0.666\n",
      "Recall:\t\t0.838\n",
      "ROC-AUC score:\t0.968\n",
      "AP-score:\t0.813\n"
     ]
    }
   ],
   "source": [
    "f_score_median_tfidf, precision_median_tfidf, recall_median_tfidf, roc_auc_score_median_tfidf, ap_score_median_tfidf = get_scores(user_embeddings_median_tfidf, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ee556-9abb-4876-8b6b-ae137a8783e1",
   "metadata": {},
   "source": [
    "#### Max, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ac23718-7bc3-4020-a4bf-a6bae7b91829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score:\t0.754\n",
      "Precision:\t0.712\n",
      "Recall:\t\t0.802\n",
      "ROC-AUC score:\t0.965\n",
      "AP-score:\t0.823\n"
     ]
    }
   ],
   "source": [
    "f_score_max_tfidf, precision_max_tfidf, recall_max_tfidf, roc_auc_score_max_tfidf, ap_score_max_tfidf = get_scores(user_embeddings_max_tfidf, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6978f9-4411-4853-9faa-3e6b5c117dc0",
   "metadata": {},
   "source": [
    "Сведём все метрики в таблицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19cb1be1-645e-409f-9fe9-199a9fd9ac9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "      <th>Max</th>\n",
       "      <th>Mean_IDF</th>\n",
       "      <th>Median_IDF</th>\n",
       "      <th>Max_IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F-score</th>\n",
       "      <td>0.634</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.559</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.733</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC-AUC</th>\n",
       "      <td>0.923</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP-score</th>\n",
       "      <td>0.626</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Mean  Median    Max  Mean_IDF  Median_IDF  Max_IDF\n",
       "F-score    0.634   0.710  0.745     0.665       0.742    0.754\n",
       "Precision  0.559   0.614  0.718     0.648       0.666    0.712\n",
       "Recall     0.733   0.842  0.773     0.684       0.838    0.802\n",
       "ROC-AUC    0.923   0.958  0.962     0.936       0.968    0.965\n",
       "AP-score   0.626   0.770  0.816     0.680       0.813    0.823"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = ['F-score', 'Precision', 'Recall', 'ROC-AUC', 'AP-score']\n",
    "columns = ['Mean', 'Median', 'Max', 'Mean_IDF', 'Median_IDF', 'Max_IDF']\n",
    "\n",
    "data = np.array([[f_score_mean, f_score_median, f_score_max, f_score_mean_tfidf, f_score_median_tfidf, f_score_max_tfidf],\n",
    "                 [precision_mean, precision_median, precision_max, precision_mean_tfidf, precision_median_tfidf, precision_max_tfidf],\n",
    "                 [recall_mean, recall_median, recall_max, recall_mean_tfidf, recall_median_tfidf, recall_max_tfidf],\n",
    "                 [roc_auc_score_mean, roc_auc_score_median, roc_auc_score_max, roc_auc_score_mean_tfidf, roc_auc_score_median_tfidf, roc_auc_score_max_tfidf],\n",
    "                 [ap_score_mean, ap_score_median, ap_score_max, ap_score_mean_tfidf, ap_score_median_tfidf, ap_score_max_tfidf]\n",
    "                ])\n",
    "df = pd.DataFrame(data, index=indices, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389104e-91af-42a4-8d18-6817f88007e2",
   "metadata": {},
   "source": [
    "Оценим дисбаланс классов целевой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6536f83-aac2-4c46-943e-fbae2a6efddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.875\n",
       "1    0.125\n",
       "Name: churn, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target['churn'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d64180-3252-4027-8460-20006cfc32df",
   "metadata": {},
   "source": [
    "<u>Выводы:</u>  \n",
    "В данной задаче метрика ROC-AUC плохо себя показала, демонстрируя \"заоблачный результат\",  \n",
    "несмотря на посредственные показатели других метрик и простоту используемой модели. Это связано с тем,  \n",
    "что доля положительного класса в выборке значительно меньше доли отрицательного, а принцип работы  \n",
    "метрики таков, что она опирается на FPR (долю ложных срабатываний относительно общего числа отрицательных  \n",
    "объектов). При большой доле отрицательных объектов FPR часто близка к нулю, а ROC-AUC, соответственно,  \n",
    "к единице. Вместо этой метрики была взята для анализа average precision score (AP-score), которая является  \n",
    "неким аналогом PR-AUC. ROC-AUC оставлена для соответствия условию домашней работы.  \n",
    "<br>\n",
    "Из таблицы выше видим, что наибольшую метрику (за основные берём F-score и AP-score) мы получаем, когда  \n",
    "берём максимальные веса тем среди 6 документов, и значительно хуже получается результат, когда за веса  \n",
    "тем пользователей берём средние значения по документам. Это происходит потому, что веса тем в документах  \n",
    "содержат много нулей, и интерес пользователя к какой-то теме нивелируется тем, что он потом читал документ,  \n",
    "не имеющий отношения к этой теме. Медиана частично решает эту проблему, не позволяя весам стремиться у нулю,  \n",
    "но и медиана имеет проблему: если 4 из 6 значений весов окажутся нулями, то пользователь как будто бы вообще  \n",
    "не интересуется темой. Использование максимального значения в нашем случае (6 документов) стало лучшим выходом,  \n",
    "хоть он и не идеален, т.к. пользователю достаточно прочесть одну новость с явным уклоном в одну из тем, и он теперь  \n",
    "считается фанатом этой темы.  \n",
    "<br>\n",
    "Использование tfidf добавило качества модели - все основные метрики увеличились. Механика весов получилась следующей:  \n",
    "веса (доли) тем каждого документа получали прибавку в зависимости от долей тем в документе и весов этих тем (содержание  \n",
    "редких/часто встречающихся слов). Таким образом, бОльшую прибавку получали документы, содержащие большую долю редких тем.  \n",
    "Нельзя сказать, почему это привело к повышению метрик, т.к. нам неизвестно, по каким  параметрам происходит отток клиентов,  \n",
    "но модель нашла какие-то зависимости."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
